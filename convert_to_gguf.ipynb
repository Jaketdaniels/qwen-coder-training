{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Fine-tuned Model to GGUF for Ollama\n",
        "\n",
        "This notebook:\n",
        "1. Merges your LoRA adapter with the base model\n",
        "2. Converts to GGUF format (Q4_K_M quantization)\n",
        "3. Creates an Ollama Modelfile\n",
        "\n",
        "**Requirements:**\n",
        "- Use **High-RAM runtime** (Runtime > Change runtime type > High-RAM)\n",
        "- T4 GPU recommended"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate peft huggingface_hub sentencepiece\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "# Clone llama.cpp for conversion scripts\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
        "!pip install -q -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to HuggingFace"
      ],
      "metadata": {
        "id": "login_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Check Memory"
      ],
      "metadata": {
        "id": "mem_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f\"Available RAM: {ram_gb:.1f} GB\")\n",
        "\n",
        "if ram_gb < 25:\n",
        "    print(\"\\n⚠️  WARNING: You need High-RAM runtime for 7B model merge!\")\n",
        "    print(\"Go to: Runtime > Change runtime type > High-RAM\")\n",
        "else:\n",
        "    print(\"✅ Sufficient RAM for model merge\")"
      ],
      "metadata": {
        "id": "check_mem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Merge LoRA with Base Model"
      ],
      "metadata": {
        "id": "merge_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "base_model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "adapter_name = \"goodknightleo/qwen-coder-7b-finetuned\"\n",
        "merged_path = \"./merged_model\"\n",
        "\n",
        "print(\"Loading base model in float16...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cpu\",  # Load on CPU to save GPU memory\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_name)\n",
        "\n",
        "print(\"Merging LoRA weights...\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"Saving merged model to {merged_path}...\")\n",
        "model.save_pretrained(merged_path, safe_serialization=True)\n",
        "tokenizer.save_pretrained(merged_path)\n",
        "\n",
        "# Free memory\n",
        "del model\n",
        "del base_model\n",
        "gc.collect()\n",
        "\n",
        "print(\"✅ Merge complete!\")"
      ],
      "metadata": {
        "id": "merge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Convert to GGUF"
      ],
      "metadata": {
        "id": "convert_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "output_gguf = \"qwen-coder-7b-finetuned.gguf\"\n",
        "\n",
        "print(\"Converting to GGUF format...\")\n",
        "result = subprocess.run([\n",
        "    \"python\", \"llama.cpp/convert_hf_to_gguf.py\",\n",
        "    merged_path,\n",
        "    \"--outfile\", output_gguf,\n",
        "    \"--outtype\", \"f16\"\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(\"Error:\", result.stderr)\n",
        "else:\n",
        "    print(f\"✅ Created {output_gguf}\")"
      ],
      "metadata": {
        "id": "convert_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Quantize to Q4_K_M (Smaller & Faster)"
      ],
      "metadata": {
        "id": "quant_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build llama.cpp quantize tool\n",
        "!cd llama.cpp && make -j quantize\n",
        "\n",
        "quantized_gguf = \"qwen-coder-7b-finetuned-Q4_K_M.gguf\"\n",
        "\n",
        "print(\"\\nQuantizing to Q4_K_M...\")\n",
        "!./llama.cpp/quantize {output_gguf} {quantized_gguf} Q4_K_M\n",
        "\n",
        "import os\n",
        "if os.path.exists(quantized_gguf):\n",
        "    size_gb = os.path.getsize(quantized_gguf) / 1e9\n",
        "    print(f\"\\n✅ Created {quantized_gguf} ({size_gb:.2f} GB)\")"
      ],
      "metadata": {
        "id": "quantize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Upload GGUF to HuggingFace"
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "repo_id = \"goodknightleo/qwen-coder-7b-finetuned-GGUF\"\n",
        "\n",
        "# Create repo\n",
        "try:\n",
        "    create_repo(repo_id, exist_ok=True)\n",
        "    print(f\"Created repo: {repo_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Repo exists or error: {e}\")\n",
        "\n",
        "# Upload quantized GGUF\n",
        "api = HfApi()\n",
        "print(f\"\\nUploading {quantized_gguf}...\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=quantized_gguf,\n",
        "    path_in_repo=quantized_gguf,\n",
        "    repo_id=repo_id,\n",
        ")\n",
        "print(f\"✅ Uploaded to https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Create Ollama Modelfile"
      ],
      "metadata": {
        "id": "modelfile_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelfile_content = '''FROM ./qwen-coder-7b-finetuned-Q4_K_M.gguf\n",
        "\n",
        "TEMPLATE \"\"\"{{- if .System }}<|im_start|>system\n",
        "{{ .System }}<|im_end|>\n",
        "{{- end }}\n",
        "<|im_start|>user\n",
        "{{ .Prompt }}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{{ .Response }}<|im_end|>\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are an expert software engineer with deep knowledge of algorithms, system design, security, and best practices. You write clean, efficient, well-documented code and can debug complex issues systematically.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"<|im_end|>\"\n",
        "PARAMETER stop \"<|im_start|>\"\n",
        "'''\n",
        "\n",
        "with open(\"Modelfile\", \"w\") as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(\"Created Modelfile:\")\n",
        "print(\"-\" * 40)\n",
        "print(modelfile_content)"
      ],
      "metadata": {
        "id": "modelfile"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Download Files\n",
        "\n",
        "Download these files to your local machine:"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading Modelfile...\")\n",
        "files.download(\"Modelfile\")\n",
        "\n",
        "print(f\"\\nDownloading {quantized_gguf}...\")\n",
        "print(\"(This may take a few minutes for a ~4GB file)\")\n",
        "files.download(quantized_gguf)"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Setup Instructions for Ollama\n",
        "\n",
        "After downloading the files, run these commands on your Mac:\n",
        "\n",
        "```bash\n",
        "# 1. Install Ollama (if not already)\n",
        "brew install ollama\n",
        "\n",
        "# 2. Start Ollama service\n",
        "ollama serve\n",
        "\n",
        "# 3. In a new terminal, create the model\n",
        "cd ~/Downloads  # or wherever you saved the files\n",
        "ollama create qwen-coder-custom -f Modelfile\n",
        "\n",
        "# 4. Run your model!\n",
        "ollama run qwen-coder-custom\n",
        "```\n",
        "\n",
        "### Use with AI Coding Tools:\n",
        "\n",
        "**Aider:**\n",
        "```bash\n",
        "pip install aider-chat\n",
        "aider --model ollama/qwen-coder-custom\n",
        "```\n",
        "\n",
        "**Continue.dev (VS Code):**\n",
        "- Install Continue extension\n",
        "- Add to config: `{\"model\": \"ollama/qwen-coder-custom\"}`"
      ],
      "metadata": {
        "id": "instructions"
      }
    }
  ]
}
