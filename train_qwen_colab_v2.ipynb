{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen2.5-Coder-7B Fine-tuning with LoRA (v2 - Fixed)\n",
        "\n",
        "This notebook fine-tunes Qwen2.5-Coder-7B-Instruct on your custom coding dataset.\n",
        "\n",
        "**Before running:**\n",
        "1. Runtime > Change runtime type > Select **T4 GPU**\n",
        "2. Have your HuggingFace token ready\n",
        "\n",
        "**Fixes in v2:**\n",
        "- Let SFTTrainer handle PEFT setup (fixes gradient issues)\n",
        "- Proper gradient checkpointing config\n",
        "- Fixed inference without corrupting merge"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers>=4.45.0 datasets accelerate>=0.30.0 peft>=0.11.0 trl>=0.9.0 bitsandbytes huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to HuggingFace"
      ],
      "metadata": {
        "id": "login_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Check GPU"
      ],
      "metadata": {
        "id": "gpu_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU detected! Go to Runtime > Change runtime type > T4 GPU\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\n",
        "    \"goodknightleo/qwen-coder-training-data\",\n",
        "    data_files=\"train.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "eval_dataset = load_dataset(\n",
        "    \"goodknightleo/qwen-coder-training-data\",\n",
        "    data_files=\"valid.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample keys: {list(train_dataset[0].keys())}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Setup Model & Tokenizer"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "\n",
        "# 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\",  # Avoid flash attention issues\n",
        ")\n",
        "\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure LoRA & Training"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Training config\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./qwen-coder-finetuned\",\n",
        "    \n",
        "    # Hub\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"goodknightleo/qwen-coder-7b-finetuned\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    \n",
        "    # Training\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    max_seq_length=1024,\n",
        "    \n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # Eval\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    \n",
        "    # Other\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    dataset_text_field=\"messages\",\n",
        ")\n",
        "\n",
        "print(\"Configuration ready!\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let SFTTrainer handle PEFT setup properly\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# Verify gradients are enabled\n",
        "trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in trainer.model.parameters())\n",
        "print(f\"Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    raise RuntimeError(\"No trainable parameters! Something is wrong.\")\n",
        "\n",
        "print(\"\\nStarting training (~20-40 min on T4)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save to Hub"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving model...\")\n",
        "trainer.save_model()\n",
        "\n",
        "print(\"Pushing to Hub...\")\n",
        "trainer.push_to_hub()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nModel: https://huggingface.co/goodknightleo/qwen-coder-7b-finetuned\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test the Model"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload for clean inference (don't merge 4-bit)\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Clear memory\n",
        "del trainer\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Loading base model for inference...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./qwen-coder-finetuned\")\n",
        "model.eval()\n",
        "\n",
        "print(\"Ready for inference!\")"
      ],
      "metadata": {
        "id": "reload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a Python function to check if a string is a palindrome.\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"Generating...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\"*50)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
